{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.utils import class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('../../data/MTeams.csv')  \n",
    "MTeam_spellings = pd.read_csv('../../data/MTeamspellings.csv', encoding='ISO-8859-1')  \n",
    "WTeam_spellings = pd.read_csv('../../data/WTeamspellings.csv', encoding='ISO-8859-1')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_spelling = pd.concat([MTeam_spellings, WTeam_spellings])\n",
    "teams_spelling = teams_spelling.groupby(by='TeamID', as_index=False)['TeamNameSpelling'].count()\n",
    "teams_spelling.columns = ['TeamID', 'TeamNameCount']\n",
    "teams = pd.merge(teams, teams_spelling, how='left', on=['TeamID'])\n",
    "del teams_spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRegularSeasonCompactResults = pd.read_csv('../../data/MRegularSeasonCompactResults.csv')\n",
    "WRegularSeasonCompactResults = pd.read_csv('../../data/WRegularSeasonCompactResults.csv')\n",
    "MRegularSeasonDetailedResults = pd.read_csv('../../data/MRegularSeasonDetailedResults.csv')\n",
    "WRegularSeasonDetailedResults = pd.read_csv('../../data/WRegularSeasonDetailedResults.csv')\n",
    "MNCAATourneyCompactResults = pd.read_csv('../../data/MNCAATourneyCompactResults.csv')\n",
    "WNCAATourneyCompactResults = pd.read_csv('../../data/WNCAATourneyCompactResults.csv')\n",
    "MNCAATourneyDetailedResults = pd.read_csv('../../data/MNCAATourneyDetailedResults.csv') \n",
    "WNCAATourneyDetailedResults = pd.read_csv('../../data/WNCAATourneyDetailedResults.csv')\n",
    "MNCAATourneySeeds = pd.read_csv('../../data/MNCAATourneySeeds.csv')\n",
    "WNCAATourneySeeds = pd.read_csv('../../data/WNCAATourneySeeds.csv')\n",
    "MGameCities = pd.read_csv('../../data/MGameCities.csv') \n",
    "WGameCities = pd.read_csv('../../data/WGameCities.csv')\n",
    "MSeasons = pd.read_csv('../../data/MSeasons.csv') \n",
    "WSeasons = pd.read_csv('../../data/WSeasons.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_cresults = pd.concat([MRegularSeasonCompactResults, WRegularSeasonCompactResults])\n",
    "season_dresults = pd.concat([MRegularSeasonDetailedResults, WRegularSeasonDetailedResults])\n",
    "tourney_cresults = pd.concat([MNCAATourneyCompactResults, WNCAATourneyCompactResults])\n",
    "tourney_dresults = pd.concat([MNCAATourneyDetailedResults, WNCAATourneyDetailedResults])\n",
    "seeds = pd.concat([MNCAATourneySeeds, WNCAATourneySeeds])\n",
    "gcities = pd.concat([MGameCities, WGameCities])\n",
    "seasons = pd.concat([MSeasons, WSeasons])\n",
    "\n",
    "seeds = {'_'.join(map(str,[int(k1),k2])):int(v[1:3]) for k1, v, k2 in seeds[['Season', 'Seed', 'TeamID']].values}\n",
    "cities = pd.read_csv('../../data/cities.csv') \n",
    "sub = pd.read_csv('../../data/SampleSubmissionStage2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_cresults['ST'] = 'S'\n",
    "season_dresults['ST'] = 'S'\n",
    "tourney_cresults['ST'] = 'T'\n",
    "tourney_dresults['ST'] = 'T'\n",
    "games = pd.concat((season_cresults, tourney_cresults), axis=0, ignore_index=True)\n",
    "games = pd.concat((season_dresults, tourney_dresults), axis=0, ignore_index=True)\n",
    "games.reset_index(drop=True, inplace=True)\n",
    "games['WLoc'] = games['WLoc'].map({'A': 1, 'H': 2, 'N': 3})\n",
    "\n",
    "games['ID'] = games.apply(lambda r: '_'.join(map(str, [r['Season']]+sorted([r['WTeamID'],r['LTeamID']]))), axis=1)\n",
    "games['IDTeams'] = games.apply(lambda r: '_'.join(map(str, sorted([r['WTeamID'],r['LTeamID']]))), axis=1)\n",
    "games['Team1'] = games.apply(lambda r: sorted([r['WTeamID'],r['LTeamID']])[0], axis=1)\n",
    "games['Team2'] = games.apply(lambda r: sorted([r['WTeamID'],r['LTeamID']])[1], axis=1)\n",
    "games['IDTeam1'] = games.apply(lambda r: '_'.join(map(str, [r['Season'], r['Team1']])), axis=1)\n",
    "games['IDTeam2'] = games.apply(lambda r: '_'.join(map(str, [r['Season'], r['Team2']])), axis=1)\n",
    "\n",
    "games['Team1Seed'] = games['IDTeam1'].map(seeds).fillna(0)\n",
    "games['Team2Seed'] = games['IDTeam2'].map(seeds).fillna(0)\n",
    "\n",
    "games['ScoreDiff'] = games['WScore'] - games['LScore']\n",
    "games['Pred'] = games.apply(lambda r: 1. if sorted([r['WTeamID'],r['LTeamID']])[0]==r['WTeamID'] else 0., axis=1)\n",
    "games['ScoreDiffNorm'] = games.apply(lambda r: r['ScoreDiff'] * -1 if r['Pred'] == 0. else r['ScoreDiff'], axis=1)\n",
    "games['SeedDiff'] = games['Team1Seed'] - games['Team2Seed'] \n",
    "games = games.fillna(-1)\n",
    "\n",
    "c_score_col = ['NumOT', 'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl',\n",
    " 'WBlk', 'WPF', 'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl',\n",
    " 'LBlk', 'LPF']\n",
    "c_score_agg = ['sum', 'mean', 'median', 'max', 'min', 'std', 'skew', 'nunique']\n",
    "gb = games.groupby(by=['IDTeams']).agg({k: c_score_agg for k in c_score_col}).reset_index()\n",
    "gb.columns = [''.join(c) + '_c_score' for c in gb.columns]\n",
    "\n",
    "games = games[games['ST']=='T']\n",
    "\n",
    "sub['WLoc'] = 3\n",
    "sub['Season'] = sub['ID'].map(lambda x: x.split('_')[0])\n",
    "sub['Season'] = sub['ID'].map(lambda x: x.split('_')[0])\n",
    "sub['Season'] = sub['Season'].astype(int)\n",
    "sub['Team1'] = sub['ID'].map(lambda x: x.split('_')[1])\n",
    "sub['Team2'] = sub['ID'].map(lambda x: x.split('_')[2])\n",
    "sub['IDTeams'] = sub.apply(lambda r: '_'.join(map(str, [r['Team1'], r['Team2']])), axis=1)\n",
    "sub['IDTeam1'] = sub.apply(lambda r: '_'.join(map(str, [r['Season'], r['Team1']])), axis=1)\n",
    "sub['IDTeam2'] = sub.apply(lambda r: '_'.join(map(str, [r['Season'], r['Team2']])), axis=1)\n",
    "sub['Team1Seed'] = sub['IDTeam1'].map(seeds).fillna(0)\n",
    "sub['Team2Seed'] = sub['IDTeam2'].map(seeds).fillna(0)\n",
    "sub['SeedDiff'] = sub['Team1Seed'] - sub['Team2Seed'] \n",
    "sub = sub.fillna(-1)\n",
    "\n",
    "games = pd.merge(games, gb, how='left', left_on='IDTeams', right_on='IDTeams_c_score')\n",
    "sub = pd.merge(sub, gb, how='left', left_on='IDTeams', right_on='IDTeams_c_score')\n",
    "\n",
    "col = [c for c in games.columns if c not in ['ID', 'DayNum', 'ST', 'Team1', 'Team2', 'IDTeams', 'IDTeam1', 'IDTeam2', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'NumOT', 'Pred', 'ScoreDiff', 'ScoreDiffNorm', 'WLoc'] + c_score_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danchizik/Desktop/marchMadness/.venv/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:48:06.839130: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-03-10 13:48:06.880745: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss (MSE): 0.2567\n",
      "  Validation Loss (MSE): 0.2699\n",
      "Epoch 2/25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:48:08.178836: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss (MSE): 0.2580\n",
      "  Validation Loss (MSE): 0.2572\n",
      "Epoch 3/25:\n",
      "  Train Loss (MSE): 0.2510\n",
      "  Validation Loss (MSE): 0.2524\n",
      "Epoch 4/25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:48:10.847954: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss (MSE): 0.2457\n",
      "  Validation Loss (MSE): 0.2512\n",
      "Epoch 5/25:\n",
      "  Train Loss (MSE): 0.2506\n",
      "  Validation Loss (MSE): 0.2510\n",
      "Epoch 6/25:\n",
      "  Train Loss (MSE): 0.2712\n",
      "  Validation Loss (MSE): 0.2557\n",
      "Epoch 7/25:\n",
      "  Train Loss (MSE): 0.2535\n",
      "  Validation Loss (MSE): 0.2526\n",
      "Epoch 8/25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:48:16.149672: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss (MSE): 0.2601\n",
      "  Validation Loss (MSE): 0.2517\n",
      "Epoch 9/25:\n",
      "  Train Loss (MSE): 0.2602\n",
      "  Validation Loss (MSE): 0.2499\n",
      "Epoch 10/25:\n",
      "  Train Loss (MSE): 0.2523\n",
      "  Validation Loss (MSE): 0.2498\n",
      "Epoch 11/25:\n",
      "  Train Loss (MSE): 0.2510\n",
      "  Validation Loss (MSE): 0.2503\n",
      "Epoch 12/25:\n",
      "  Train Loss (MSE): 0.2526\n",
      "  Validation Loss (MSE): 0.2516\n",
      "Epoch 13/25:\n",
      "  Train Loss (MSE): 0.2496\n",
      "  Validation Loss (MSE): 0.2497\n",
      "Epoch 14/25:\n",
      "  Train Loss (MSE): 0.2679\n",
      "  Validation Loss (MSE): 0.2639\n",
      "Epoch 15/25:\n",
      "  Train Loss (MSE): 0.2489\n",
      "  Validation Loss (MSE): 0.2507\n",
      "Epoch 16/25:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-10 13:48:26.882686: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss (MSE): 0.2236\n",
      "  Validation Loss (MSE): 0.2580\n",
      "Epoch 17/25:\n",
      "  Train Loss (MSE): 0.2493\n",
      "  Validation Loss (MSE): 0.2498\n",
      "Epoch 18/25:\n",
      "  Train Loss (MSE): 0.2487\n",
      "  Validation Loss (MSE): 0.2498\n",
      "Epoch 19/25:\n",
      "  Train Loss (MSE): 0.2706\n",
      "  Validation Loss (MSE): 0.2506\n",
      "Epoch 20/25:\n",
      "  Train Loss (MSE): 0.2473\n",
      "  Validation Loss (MSE): 0.2506\n",
      "Epoch 21/25:\n",
      "  Train Loss (MSE): 0.2491\n",
      "  Validation Loss (MSE): 0.2511\n",
      "Epoch 22/25:\n",
      "  Train Loss (MSE): 0.2540\n",
      "  Validation Loss (MSE): 0.2497\n",
      "Epoch 23/25:\n",
      "  Train Loss (MSE): 0.2454\n",
      "  Validation Loss (MSE): 0.2551\n",
      "Epoch 24/25:\n",
      "  Train Loss (MSE): 0.2498\n",
      "  Validation Loss (MSE): 0.2499\n",
      "Epoch 25/25:\n",
      "  Train Loss (MSE): 0.2475\n",
      "  Validation Loss (MSE): 0.2514\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Data Preprocessing\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = games[col].fillna(-1)\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "y = games['Pred']\n",
    "\n",
    "# Convert to TensorFlow format\n",
    "X_train_tensor = tf.convert_to_tensor(X_scaled, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "\n",
    "# Train, valid split\n",
    "train_size = int(0.8 * len(X_train_tensor))  # 80% for training\n",
    "val_size = len(X_train_tensor) - train_size  # Remaining 20% for validation\n",
    "\n",
    "X_train, X_val = X_train_tensor[:train_size], X_train_tensor[train_size:]\n",
    "y_train, y_val = y_train_tensor[:train_size], y_train_tensor[train_size:]\n",
    "\n",
    "# Create the TensorFlow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# Neural Network Model\n",
    "class NeuralNetwork(keras.Model):\n",
    "    def __init__(self, d_in, d_out, d_hidden, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.d_hidden = d_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.dense1 = layers.Dense(d_hidden, activation='relu', input_dim=d_in)\n",
    "        self.batch_norm1 = layers.BatchNormalization()\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        \n",
    "        self.hidden_layers = [layers.Dense(d_hidden, activation='relu') for _ in range(n_layers)]\n",
    "        self.batch_norms = [layers.BatchNormalization() for _ in range(n_layers)]\n",
    "        \n",
    "        self.output_layer = layers.Dense(d_out)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.batch_norm1(x)\n",
    "        \n",
    "        for dense, batch_norm in zip(self.hidden_layers, self.batch_norms):\n",
    "            x = dense(x)\n",
    "            x = batch_norm(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Model creation\n",
    "model = NeuralNetwork(X_scaled.shape[1], 1, 100)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "\n",
    "# Training function\n",
    "def train(model, train_dataset, val_dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        # Training loop\n",
    "        for batch_X, batch_y in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(batch_X)\n",
    "                loss = tf.reduce_mean(tf.square(batch_y - predictions))\n",
    "                \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "        # Validation loop\n",
    "        val_loss = 0\n",
    "        for batch_X, batch_y in val_dataset:\n",
    "            predictions = model(batch_X)\n",
    "            val_loss += tf.reduce_mean(tf.square(batch_y - predictions))\n",
    "        \n",
    "        print(f\"  Train Loss (MSE): {loss.numpy():.4f}\")\n",
    "        print(f\"  Validation Loss (MSE): {val_loss.numpy()/len(val_dataset):.4f}\")\n",
    "\n",
    "# Train model\n",
    "train(model, train_dataset, val_dataset, epochs=25)\n",
    "\n",
    "# Prepare for submission\n",
    "X_submit = sub[col].fillna(-1)\n",
    "X_submit_imputed = imputer.transform(X_submit)\n",
    "X_submit_scaled = scaler.transform(X_submit_imputed)\n",
    "\n",
    "X_submit_tensor = tf.convert_to_tensor(X_submit_scaled, dtype=tf.float32)\n",
    "\n",
    "# Predicting on submission data\n",
    "y_preds = model(X_submit_tensor).numpy()\n",
    "\n",
    "sub['Pred'] = y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114630</th>\n",
       "      <td>2025_3288_3351</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>2025_1123_1320</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26418</th>\n",
       "      <td>2025_1188_1454</td>\n",
       "      <td>0.524479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20227</th>\n",
       "      <td>2025_1167_1392</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>2025_1117_1192</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121104</th>\n",
       "      <td>2025_3330_3474</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13084</th>\n",
       "      <td>2025_1143_1474</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129183</th>\n",
       "      <td>2025_3409_3468</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55366</th>\n",
       "      <td>2025_1329_1361</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12439</th>\n",
       "      <td>2025_1142_1144</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41511</th>\n",
       "      <td>2025_1250_1457</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39359</th>\n",
       "      <td>2025_1241_1334</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99985</th>\n",
       "      <td>2025_3220_3430</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118840</th>\n",
       "      <td>2025_3314_3475</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112829</th>\n",
       "      <td>2025_3278_3427</td>\n",
       "      <td>0.518651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27222</th>\n",
       "      <td>2025_1191_1415</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108603</th>\n",
       "      <td>2025_3257_3468</td>\n",
       "      <td>0.524479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46920</th>\n",
       "      <td>2025_1276_1443</td>\n",
       "      <td>0.517889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114100</th>\n",
       "      <td>2025_3285_3374</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94809</th>\n",
       "      <td>2025_3198_3468</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72886</th>\n",
       "      <td>2025_3123_3262</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78280</th>\n",
       "      <td>2025_3141_3322</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34471</th>\n",
       "      <td>2025_1221_1253</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99415</th>\n",
       "      <td>2025_3218_3361</td>\n",
       "      <td>0.524899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50461</th>\n",
       "      <td>2025_1296_1452</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122946</th>\n",
       "      <td>2025_3344_3401</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79505</th>\n",
       "      <td>2025_3145_3251</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>2025_1117_1191</td>\n",
       "      <td>0.525148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96940</th>\n",
       "      <td>2025_3206_3467</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38678</th>\n",
       "      <td>2025_1238_1349</td>\n",
       "      <td>0.538426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID      Pred\n",
       "114630  2025_3288_3351  0.538426\n",
       "6915    2025_1123_1320  0.538426\n",
       "26418   2025_1188_1454  0.524479\n",
       "20227   2025_1167_1392  0.538426\n",
       "5409    2025_1117_1192  0.538426\n",
       "121104  2025_3330_3474  0.538426\n",
       "13084   2025_1143_1474  0.538426\n",
       "129183  2025_3409_3468  0.538426\n",
       "55366   2025_1329_1361  0.538426\n",
       "12439   2025_1142_1144  0.538426\n",
       "41511   2025_1250_1457  0.538426\n",
       "39359   2025_1241_1334  0.538426\n",
       "99985   2025_3220_3430  0.538426\n",
       "118840  2025_3314_3475  0.538426\n",
       "112829  2025_3278_3427  0.518651\n",
       "27222   2025_1191_1415  0.538426\n",
       "108603  2025_3257_3468  0.524479\n",
       "46920   2025_1276_1443  0.517889\n",
       "114100  2025_3285_3374  0.538426\n",
       "94809   2025_3198_3468  0.538426\n",
       "72886   2025_3123_3262  0.538426\n",
       "78280   2025_3141_3322  0.538426\n",
       "34471   2025_1221_1253  0.538426\n",
       "99415   2025_3218_3361  0.524899\n",
       "50461   2025_1296_1452  0.538426\n",
       "122946  2025_3344_3401  0.538426\n",
       "79505   2025_3145_3251  0.538426\n",
       "5408    2025_1117_1191  0.525148\n",
       "96940   2025_3206_3467  0.538426\n",
       "38678   2025_1238_1349  0.538426"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[['ID', 'Pred']].sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
